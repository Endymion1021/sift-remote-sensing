{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "modules & envs"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import cv2\n",
    "import math\n",
    "import os,sys\n",
    "import scipy.ndimage\n",
    "import time\n",
    "import scipy\n",
    "from numpy.linalg import det, lstsq, norm\n",
    "from functools import cmp_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "首先，定义一个CSift类，用于传递SIFT所需的多个参数\n",
    "'''\n",
    "class  CSift: \n",
    "\tdef __init__(self,num_octave,sigma): \n",
    "\t\t'''\n",
    "\t\t调用CSift类的时候，除初始化的一般参数外，还需要传入三个参数：\n",
    "\t\tnum_octave: 高斯金字塔的组数\n",
    "\t\tsigma: 标准差，每一层对应的都不同\n",
    "\t\t'''\n",
    "\t\tself.sigma = sigma\t# 初始尺度因子\n",
    "\t\tself.num_octave = 3 # 组数，后续重新计算\n",
    "\t\t'''\n",
    "\t\t以下参数为常量\n",
    "\t\t'''\n",
    "\t\tself.num_scale = 3 # 根据Lowe建议选择\n",
    "\t\tself.contrast_t = 0.04 # 弱响应阈值\n",
    "\t\tself.eigenvalue_r = 10 # hessian矩阵特征值的比值阈值\n",
    "\t\tself.scale_factor = 1.5 # 求取方位信息时的尺度系数\n",
    "\t\tself.radius_factor = 3 # 被采样率\n",
    "\t\tself.num_bins = 36 # 极值点方位方向划分\n",
    "\t\tself.peak_ratio = 0.8 # 极值点方向分配时，辅方向的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.构建尺度空间：图像降采样+高斯模糊\n",
    "\n",
    "Lowe论文原文：Of course, if we pre-smooth the image before extrema detection, we are effectively discarding the highest spatial frequencies. Therefore, to make full use of the input, the image can be expanded to <u>create more sample points</u> than were present in the original. We **double the size of the input image** using <u>*linear interpolation*</u> prior to building the first level of the pyramidamid.\n",
    "\n",
    "首先通过cv2.resize方法，对原始图像进行2x缩放+高斯模糊，作为高斯金字塔的底层图像。目的是充分利用图像的空间结构信息。\n",
    "\n",
    "效果：最终匹配得到的特征点对数量，提升四倍之多！\n",
    "\n",
    "注：cv2.resize插值方法：\n",
    "+ INTER_NEAREST 最近邻插值\n",
    "+ INTER_LINEAR 双线性插值（默认设置）\n",
    "+ INTER_AREA 使用像素区域关系进行重采样。\n",
    "+ INTER_CUBIC 4x4像素邻域的双三次插值\n",
    "+ INTER_LANCZOS4 8x8像素邻域的Lanczos插值\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得高斯金字塔的底层图像\n",
    "def get_base_img(img_src,sigma,cam_blur=0.5):\n",
    "    '''\n",
    "    img_src:输入原始图像\n",
    "    sigma:尺度因子\n",
    "    cam_blur: Lowe在论文中提出,原始图像的最小原始模糊sigma=0.5,防止混叠所需达到的最小值\n",
    "    在这一步操作中，进行高斯模糊操作时需要注意折算cam_blur的效果（根据高斯模糊的性质）\n",
    "    '''\n",
    "    sigma_cor = np.sqrt(sigma**2 - (2*cam_blur)**2) # 2x缩放后的图像有2x的像素间隔，所以cam_blur也为2x\n",
    "    img = img_src.copy()\n",
    "    img = cv2.resize(img,dsize=None,fx=2,fy=2,interpolation=cv2.INTER_LINEAR) # cv2.resize 2x缩放\n",
    "    img = cv2.GaussianBlur(img,ksize=None,sigmaX=sigma_cor,sigmaY=sigma_cor) # 对缩放后图像进行高斯模糊\n",
    "    # 以折算后的尺度因子生成高斯卷积核，cv2.GaussianBlur方法调用了:\n",
    "    # 1. cv2.GetGaussianKernal，分别生成两个一维高斯卷积核KernalX、KernalY；\n",
    "    # 2. cv2.sepFilter2D，用kernalX对图像行做卷积，KernalY对图像列做卷积；\n",
    "    # 最后对图像做归一化处理。\n",
    "    return img # 得到底层图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算高斯金字塔的组数\n",
    "def get_numOfOctave(img):\n",
    "\tnum = round(np.log(min(img.shape[0],img.shape[1]))/np.log(2))-1 \n",
    "\t# o = log{2}{min(x,y)}-t，其中t=log{2}{min(x,y)(顶层图像)}\n",
    "\t# 本例中底层的图像尺寸为1220*1880，顶层图像尺寸为2*4, 因此t=1\n",
    "\treturn num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建高斯金字塔\n",
    "def construct_octave(img_src,s,sigma):\n",
    "    '''\n",
    "    s: 对单个octave的划分，对图像进行高斯模糊的期望次数\n",
    "    '''\n",
    "    octave = []\n",
    "    octave.append(img_src)\n",
    "    k = 2**(1/s) # octave的每个划分之间的尺度系数，保证顶层和底层的2x尺度关系\n",
    "    for i in range(1,s+3): \n",
    "        '''\n",
    "        这里进行了s+3次尺度滤波，因为Lowe认为额外的三次滤波的意义，是\"so that final extrema detection covers a complete octave.\"\n",
    "        但是3从何而来?s个极值点检测结果需要s+2个差分层(头尾各+1)，需要(s+2)+1个高斯滤波层。\n",
    "        可以发现，进行到s+1次滤波时，对应的sigma就是2x本octave的sigma_0，正好是下一octave的sigma_0\n",
    "        根据Lowe在原文中给出的建议，选取s=3\n",
    "        '''\n",
    "        img = octave[-1].copy()\n",
    "        cur_sigma = k**i*sigma # 当前尺度 \n",
    "        pre_sigma = k**(i-1)*sigma # 前一图像尺度\n",
    "        true_sigma = np.sqrt(cur_sigma**2-pre_sigma**2) # 折算后本层对应尺度因子\n",
    "        blur_img = cv2.GaussianBlur(img,ksize=None,sigmaX=true_sigma,sigmaY=true_sigma)\n",
    "        octave.append(blur_img)\n",
    "    return octave\n",
    "\n",
    "def construct_gaussian_pyramid(img_src,sift:CSift):\n",
    "    '''\n",
    "    高斯金字塔的部分关键参数，通过CSift类传递\n",
    "    '''\n",
    "    pyramid=[]\n",
    "    img_base = img_src.copy()\n",
    "    for i in range(sift.num_octave):#共计构建octave组\n",
    "        octave = construct_octave(img_base,sift.num_scale,sift.sigma) \n",
    "        pyramid.append(octave)\n",
    "        img_base = octave[-3]#倒数第三层的尺度与下一组的初始尺度相同，对该层进行降采样，作为下一组的图像输入\n",
    "        img_base = cv2.resize(img_base,(int(img_base.shape[1]/2),int(img_base.shape[0]/2)),interpolation=cv2.INTER_NEAREST)\n",
    "    return pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_match(img_src1,kp1,des1,img_src2,kp2,des2,embed=1,pt_flag=0,MIN_MATCH_COUNT = 10):\n",
    "\t## 1. 对关键点进行匹配 ##\n",
    "\tFLANN_INDEX_KDTREE = 0\n",
    "\tindex_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "\tsearch_params = dict(checks=50)\n",
    "\tflann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\tdes1, des2 = np.array(des1).astype(np.float32), np.array(des2).astype(np.float32)#需要转成array\n",
    "\tmatches = flann.knnMatch(des1, des2, k=2)  # matches为list，每个list元素由2个DMatch类型变量组成,分别是最邻近和次邻近点\n",
    "\n",
    "\tgood_match = []\n",
    "\tfor m in matches:\n",
    "\t\tif m[0].distance < 0.7 * m[1].distance:  # 如果最邻近和次邻近的距离差距较大,则认可\n",
    "\t\t\tgood_match.append(m[0])\n",
    "\t## 2. 将2张图画在同一张图上 ##\n",
    "\timg1 = img_src1.copy()\n",
    "\timg2 = img_src2.copy()\n",
    "\th1, w1 = img1.shape[0],img1.shape[1]\n",
    "\th2, w2 = img2.shape[0],img2.shape[1]\n",
    "\tnew_w = w1 + w2\n",
    "\tnew_h = np.max([h1, h2])\n",
    "\tnew_img =  np.zeros((new_h, new_w,3), np.uint8) if len(img_src1.shape)==3 else  np.zeros((new_h, new_w), np.uint8)\n",
    "\th_offset1 = int(0.5 * (new_h - h1))\n",
    "\th_offset2 = int(0.5 * (new_h - h2))\n",
    "\tif len(img_src1.shape) == 3:\n",
    "\t\tnew_img[h_offset1:h_offset1 + h1, :w1,:] = img1  # 左边画img1\n",
    "\t\tnew_img[h_offset2:h_offset2 + h2, w1:w1 + w2,:] = img2  # 右边画img2\n",
    "\telse:\n",
    "\t\tnew_img[h_offset1:h_offset1 + h1, :w1] = img1  # 左边画img1\n",
    "\t\tnew_img[h_offset2:h_offset2 + h2, w1:w1 + w2] = img2  # 右边画img2\n",
    "\t##3. 两幅图存在足够的匹配点，两幅图匹配成功，将匹配成功的关键点进行连线 ##\n",
    "\tif len(good_match) > MIN_MATCH_COUNT:\n",
    "\t\tsrc_pts = []\n",
    "\t\tdst_pts = []\n",
    "\t\tmag_err_arr=[]\n",
    "\t\tangle_err_arr=[]\n",
    "\t\tfor m in good_match:\n",
    "\t\t\tif pt_flag==0:#point是百分比\n",
    "\t\t\t\tsrc_pts.append([kp1[m.queryIdx].pt[0] * img1.shape[1], kp1[m.queryIdx].pt[1] * img1.shape[0]])#保存匹配成功的原图关键点位置\n",
    "\t\t\t\tdst_pts.append([kp2[m.trainIdx].pt[0] * img2.shape[1], kp2[m.trainIdx].pt[1] * img2.shape[0]])#保存匹配成功的目标图关键点位置\n",
    "\t\t\telse:\n",
    "\t\t\t\tsrc_pts.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1]])  # 保存匹配成功的原图关键点位置\n",
    "\t\t\t\tdst_pts.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1]])  # 保存匹配成功的目标图关键点位置\n",
    "\n",
    "\t\t\tmag_err = np.abs(kp1[m.queryIdx].response - kp2[m.trainIdx].response) / np.abs(kp1[m.queryIdx].response )\n",
    "\t\t\tangle_err = np.abs(kp1[m.queryIdx].angle - kp2[m.trainIdx].angle)\n",
    "\t\t\tmag_err_arr.append(mag_err)\n",
    "\t\t\tangle_err_arr.append(angle_err)\n",
    "\n",
    "\t\tif embed!=0 :#若图像2是图像1内嵌入另一个大的背景中，则在图像2中，突出显示图像1的边界\n",
    "\t\t\tM = cv2.findHomography(np.array(src_pts), np.array(dst_pts), cv2.RANSAC, 5.0)[0]  # 根据src和dst关键点，寻求变换矩阵\n",
    "\t\t\tsrc_w, src_h = img1.shape[1], img1.shape[0]\n",
    "\t\t\tsrc_rect = np.array([[0, 0], [src_w - 1, 0], [src_w - 1, src_h - 1], [0, src_h - 1]]).reshape(-1, 1, 2).astype(\n",
    "\t\t\t\tnp.float32)  # 原始图像的边界框\n",
    "\t\t\tdst_rect = cv2.perspectiveTransform(src_rect, M)  # 经映射后，得到dst的边界框\n",
    "\t\t\timg2 = cv2.polylines(img2, [np.int32(dst_rect)], True, 255, 3, cv2.LINE_AA)  # 将边界框画在dst图像上，突出显示\n",
    "\t\t\tif len(new_img.shape) == 3:\n",
    "\t\t\t\tnew_img[h_offset2:h_offset2 + h2, w1:w1 + w2,:] = img2  # 右边画img2\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_img[h_offset2:h_offset2 + h2, w1:w1 + w2] = img2  # 右边画img2\n",
    "\n",
    "\t\tnew_img = new_img if len(new_img.shape) == 3 else  cv2.cvtColor(new_img, cv2.COLOR_GRAY2BGR)\n",
    "\t\t# 连线\n",
    "\t\tfor pt1, pt2 in zip(src_pts, dst_pts):\n",
    "\t\t\tcv2.line(new_img, tuple(np.int32(np.array(pt1) + [0, h_offset1])),\n",
    "\t\t\t\t\t tuple(np.int32(np.array(pt2) + [w1, h_offset2])), color=(0, 0, 255))\n",
    "\treturn new_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用OpenCV库中的cv2.SIFT.create方法实现SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\tMIN_MATCH_COUNT = 10\n",
    "\tsift = CSift(num_octave=4,num_scale=3,sigma=1.6)\n",
    "\timg_src1 = cv2.imread('D:/Projects/SIFT Matching/remote_sensing_pair/image1.bmp',-1)\n",
    "\t#img_src1 = cv2.resize(img_src1, (0, 0), fx=.25, fy=.25)\n",
    "\timg_src2 = cv2.imread('D:/Projects/SIFT Matching/remote_sensing_pair/image2.bmp', -1)\n",
    "\t#img_src2 = cv2.resize(img_src2, (0, 0), fx=.5, fy=.5)\n",
    "\t# 2. 使用opencv自带sift算子\n",
    "\tsift.num_octave = get_numOfOctave(img_src1)\n",
    "\topencv_sift = cv2.SIFT.create(nfeatures=None, nOctaveLayers=sift.num_octave,\n",
    "\t\t\t\t\t\t\t\t  contrastThreshold=sift.contrast_t, edgeThreshold=sift.eigenvalue_r, sigma=sift.sigma)\n",
    "\tkp1 = opencv_sift.detect(img_src1)\n",
    "\tkp1,des1 = opencv_sift.compute(img_src1,kp1)\n",
    "\n",
    "\tsift.num_octave = get_numOfOctave(img_src2)\n",
    "\topencv_sift = cv2.SIFT.create(nfeatures=None, nOctaveLayers=sift.num_octave,\n",
    "\t\t\t\t\t\t\t\t  contrastThreshold=sift.contrast_t, edgeThreshold=sift.eigenvalue_r, sigma=sift.sigma)\n",
    "\tkp2 = opencv_sift.detect(img_src2)\n",
    "\tkp2, des2 = opencv_sift.compute(img_src2, kp2)\n",
    "\tpt_flag = 1\n",
    "\n",
    "\t# 3. 做匹配\n",
    "\treu_img = do_match(img_src1, kp1, des1, img_src2, kp2, des2, embed=1, pt_flag=pt_flag,MIN_MATCH_COUNT=3)\n",
    "\tcv2.imshow('reu',reu_img)\n",
    "\tcv2.imwrite('reu.tif',reu_img)\n",
    "\t'''\n",
    "\t最初cv2.imshow方法弹出的图像显示窗口无响应，查阅资料后增加以下办法，\n",
    "\t通过调用destroyWindows()函数来释放由OpenCV创建的所有窗口\n",
    "\t'''\n",
    "\tcv2.waitKey(1000)\n",
    "\tcv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
